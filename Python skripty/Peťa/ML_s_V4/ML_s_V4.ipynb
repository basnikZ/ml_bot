{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4291d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "Epoch 1/100\n",
      "10/10 [==============================] - 11s 488ms/step - loss: 0.1107 - rmse: 0.3327 - val_loss: 0.0290 - val_rmse: 0.1703\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 4s 443ms/step - loss: 0.0173 - rmse: 0.1316 - val_loss: 0.0081 - val_rmse: 0.0901\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 5s 468ms/step - loss: 0.0083 - rmse: 0.0914 - val_loss: 0.0086 - val_rmse: 0.0929\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 5s 455ms/step - loss: 0.0064 - rmse: 0.0800 - val_loss: 0.0045 - val_rmse: 0.0674\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 5s 480ms/step - loss: 0.0056 - rmse: 0.0748 - val_loss: 0.0050 - val_rmse: 0.0711\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 5s 517ms/step - loss: 0.0057 - rmse: 0.0752 - val_loss: 0.0041 - val_rmse: 0.0638\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 5s 552ms/step - loss: 0.0053 - rmse: 0.0726 - val_loss: 0.0045 - val_rmse: 0.0672\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 5s 528ms/step - loss: 0.0050 - rmse: 0.0710 - val_loss: 0.0043 - val_rmse: 0.0654\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 5s 537ms/step - loss: 0.0046 - rmse: 0.0675 - val_loss: 0.0036 - val_rmse: 0.0600\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 5s 510ms/step - loss: 0.0048 - rmse: 0.0690 - val_loss: 0.0036 - val_rmse: 0.0600\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 5s 494ms/step - loss: 0.0049 - rmse: 0.0702 - val_loss: 0.0041 - val_rmse: 0.0641\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 5s 512ms/step - loss: 0.0039 - rmse: 0.0622 - val_loss: 0.0032 - val_rmse: 0.0566\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 5s 531ms/step - loss: 0.0043 - rmse: 0.0654 - val_loss: 0.0036 - val_rmse: 0.0602\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 5s 501ms/step - loss: 0.0038 - rmse: 0.0617 - val_loss: 0.0043 - val_rmse: 0.0658\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 5s 520ms/step - loss: 0.0044 - rmse: 0.0661 - val_loss: 0.0029 - val_rmse: 0.0543\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 5s 510ms/step - loss: 0.0039 - rmse: 0.0624 - val_loss: 0.0030 - val_rmse: 0.0552\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 5s 536ms/step - loss: 0.0039 - rmse: 0.0628 - val_loss: 0.0032 - val_rmse: 0.0566\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 5s 528ms/step - loss: 0.0040 - rmse: 0.0632 - val_loss: 0.0029 - val_rmse: 0.0536\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 0.0035 - rmse: 0.0588 - val_loss: 0.0027 - val_rmse: 0.0516\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 6s 566ms/step - loss: 0.0039 - rmse: 0.0622 - val_loss: 0.0027 - val_rmse: 0.0518\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 6s 571ms/step - loss: 0.0042 - rmse: 0.0647 - val_loss: 0.0036 - val_rmse: 0.0597\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 6s 603ms/step - loss: 0.0042 - rmse: 0.0648 - val_loss: 0.0026 - val_rmse: 0.0511\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 6s 567ms/step - loss: 0.0037 - rmse: 0.0609 - val_loss: 0.0025 - val_rmse: 0.0497\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 6s 574ms/step - loss: 0.0041 - rmse: 0.0637 - val_loss: 0.0026 - val_rmse: 0.0510\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 6s 587ms/step - loss: 0.0040 - rmse: 0.0629 - val_loss: 0.0024 - val_rmse: 0.0488\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 6s 562ms/step - loss: 0.0040 - rmse: 0.0632 - val_loss: 0.0030 - val_rmse: 0.0543\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 0.0038 - rmse: 0.0618 - val_loss: 0.0030 - val_rmse: 0.0548\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 6s 591ms/step - loss: 0.0034 - rmse: 0.0584 - val_loss: 0.0023 - val_rmse: 0.0479\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 6s 574ms/step - loss: 0.0039 - rmse: 0.0623 - val_loss: 0.0026 - val_rmse: 0.0505\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 6s 558ms/step - loss: 0.0034 - rmse: 0.0582 - val_loss: 0.0023 - val_rmse: 0.0482\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 6s 561ms/step - loss: 0.0035 - rmse: 0.0593 - val_loss: 0.0023 - val_rmse: 0.0477\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 6s 582ms/step - loss: 0.0040 - rmse: 0.0634 - val_loss: 0.0023 - val_rmse: 0.0481\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 6s 570ms/step - loss: 0.0048 - rmse: 0.0690 - val_loss: 0.0027 - val_rmse: 0.0520\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 6s 591ms/step - loss: 0.0042 - rmse: 0.0645 - val_loss: 0.0025 - val_rmse: 0.0497\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 6s 598ms/step - loss: 0.0043 - rmse: 0.0653 - val_loss: 0.0023 - val_rmse: 0.0480\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 6s 600ms/step - loss: 0.0040 - rmse: 0.0633 - val_loss: 0.0025 - val_rmse: 0.0496\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 6s 594ms/step - loss: 0.0039 - rmse: 0.0626 - val_loss: 0.0023 - val_rmse: 0.0475\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 6s 564ms/step - loss: 0.0033 - rmse: 0.0578 - val_loss: 0.0025 - val_rmse: 0.0501\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 0.0034 - rmse: 0.0587 - val_loss: 0.0022 - val_rmse: 0.0464\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 0.0038 - rmse: 0.0620 - val_loss: 0.0031 - val_rmse: 0.0559\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 5s 537ms/step - loss: 0.0034 - rmse: 0.0582 - val_loss: 0.0029 - val_rmse: 0.0539\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 5s 536ms/step - loss: 0.0032 - rmse: 0.0561 - val_loss: 0.0026 - val_rmse: 0.0510\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 5s 526ms/step - loss: 0.0036 - rmse: 0.0602 - val_loss: 0.0025 - val_rmse: 0.0503\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 5s 534ms/step - loss: 0.0032 - rmse: 0.0569 - val_loss: 0.0021 - val_rmse: 0.0454\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 5s 536ms/step - loss: 0.0033 - rmse: 0.0576 - val_loss: 0.0023 - val_rmse: 0.0483\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 5s 528ms/step - loss: 0.0033 - rmse: 0.0574 - val_loss: 0.0026 - val_rmse: 0.0507\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 5s 532ms/step - loss: 0.0033 - rmse: 0.0573 - val_loss: 0.0022 - val_rmse: 0.0474\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 5s 537ms/step - loss: 0.0031 - rmse: 0.0553 - val_loss: 0.0024 - val_rmse: 0.0491\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 5s 496ms/step - loss: 0.0034 - rmse: 0.0583 - val_loss: 0.0020 - val_rmse: 0.0453\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - 5s 492ms/step - loss: 0.0035 - rmse: 0.0592 - val_loss: 0.0021 - val_rmse: 0.0456\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - 5s 497ms/step - loss: 0.0033 - rmse: 0.0573 - val_loss: 0.0020 - val_rmse: 0.0444\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - 5s 491ms/step - loss: 0.0034 - rmse: 0.0584 - val_loss: 0.0019 - val_rmse: 0.0438\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - 5s 489ms/step - loss: 0.0032 - rmse: 0.0562 - val_loss: 0.0019 - val_rmse: 0.0440\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - 5s 489ms/step - loss: 0.0032 - rmse: 0.0563 - val_loss: 0.0020 - val_rmse: 0.0443\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - 5s 493ms/step - loss: 0.0037 - rmse: 0.0606 - val_loss: 0.0020 - val_rmse: 0.0445\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - 5s 489ms/step - loss: 0.0036 - rmse: 0.0601 - val_loss: 0.0020 - val_rmse: 0.0445\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - 5s 489ms/step - loss: 0.0031 - rmse: 0.0555 - val_loss: 0.0019 - val_rmse: 0.0439\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - 5s 499ms/step - loss: 0.0033 - rmse: 0.0571 - val_loss: 0.0021 - val_rmse: 0.0459\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - 5s 493ms/step - loss: 0.0035 - rmse: 0.0589 - val_loss: 0.0025 - val_rmse: 0.0504\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - 5s 493ms/step - loss: 0.0033 - rmse: 0.0574 - val_loss: 0.0022 - val_rmse: 0.0464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "10/10 [==============================] - 5s 500ms/step - loss: 0.0032 - rmse: 0.0567 - val_loss: 0.0025 - val_rmse: 0.0498\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - 5s 492ms/step - loss: 0.0028 - rmse: 0.0527 - val_loss: 0.0022 - val_rmse: 0.0464\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - 5s 493ms/step - loss: 0.0030 - rmse: 0.0547 - val_loss: 0.0022 - val_rmse: 0.0471\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - 5s 500ms/step - loss: 0.0030 - rmse: 0.0551 - val_loss: 0.0019 - val_rmse: 0.0438\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - 5s 498ms/step - loss: 0.0036 - rmse: 0.0600 - val_loss: 0.0019 - val_rmse: 0.0437\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - 5s 497ms/step - loss: 0.0033 - rmse: 0.0576 - val_loss: 0.0020 - val_rmse: 0.0442\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - 5s 503ms/step - loss: 0.0036 - rmse: 0.0602 - val_loss: 0.0022 - val_rmse: 0.0469\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - 5s 504ms/step - loss: 0.0039 - rmse: 0.0628 - val_loss: 0.0042 - val_rmse: 0.0649\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - 5s 507ms/step - loss: 0.0040 - rmse: 0.0632 - val_loss: 0.0030 - val_rmse: 0.0546\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - 5s 508ms/step - loss: 0.0033 - rmse: 0.0572 - val_loss: 0.0021 - val_rmse: 0.0457\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - 5s 516ms/step - loss: 0.0029 - rmse: 0.0542 - val_loss: 0.0018 - val_rmse: 0.0430\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - 5s 507ms/step - loss: 0.0031 - rmse: 0.0558 - val_loss: 0.0019 - val_rmse: 0.0436\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - 5s 502ms/step - loss: 0.0031 - rmse: 0.0561 - val_loss: 0.0020 - val_rmse: 0.0449\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - 5s 511ms/step - loss: 0.0034 - rmse: 0.0579 - val_loss: 0.0020 - val_rmse: 0.0447\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - 5s 502ms/step - loss: 0.0035 - rmse: 0.0588 - val_loss: 0.0019 - val_rmse: 0.0436\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - 5s 504ms/step - loss: 0.0037 - rmse: 0.0605 - val_loss: 0.0023 - val_rmse: 0.0477\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - 5s 516ms/step - loss: 0.0032 - rmse: 0.0562 - val_loss: 0.0019 - val_rmse: 0.0435\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - 5s 502ms/step - loss: 0.0030 - rmse: 0.0551 - val_loss: 0.0034 - val_rmse: 0.0583\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - 5s 507ms/step - loss: 0.0033 - rmse: 0.0571 - val_loss: 0.0050 - val_rmse: 0.0708\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - 5s 544ms/step - loss: 0.0040 - rmse: 0.0635 - val_loss: 0.0045 - val_rmse: 0.0670\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - 5s 507ms/step - loss: 0.0036 - rmse: 0.0602 - val_loss: 0.0049 - val_rmse: 0.0703\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - 5s 509ms/step - loss: 0.0035 - rmse: 0.0589 - val_loss: 0.0028 - val_rmse: 0.0527\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - 5s 511ms/step - loss: 0.0034 - rmse: 0.0581 - val_loss: 0.0020 - val_rmse: 0.0442\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - 5s 508ms/step - loss: 0.0033 - rmse: 0.0571 - val_loss: 0.0021 - val_rmse: 0.0456\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - 5s 510ms/step - loss: 0.0033 - rmse: 0.0578 - val_loss: 0.0023 - val_rmse: 0.0481\n",
      "Epoch 86/100\n",
      "10/10 [==============================] - 5s 522ms/step - loss: 0.0029 - rmse: 0.0541 - val_loss: 0.0024 - val_rmse: 0.0485\n",
      "Epoch 87/100\n",
      "10/10 [==============================] - 5s 510ms/step - loss: 0.0031 - rmse: 0.0556 - val_loss: 0.0020 - val_rmse: 0.0450\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - 5s 503ms/step - loss: 0.0027 - rmse: 0.0520 - val_loss: 0.0019 - val_rmse: 0.0431\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - 5s 507ms/step - loss: 0.0029 - rmse: 0.0537 - val_loss: 0.0018 - val_rmse: 0.0426\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - 5s 506ms/step - loss: 0.0033 - rmse: 0.0576 - val_loss: 0.0017 - val_rmse: 0.0416\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - 5s 507ms/step - loss: 0.0029 - rmse: 0.0536 - val_loss: 0.0021 - val_rmse: 0.0456\n",
      "Epoch 92/100\n",
      "10/10 [==============================] - 5s 518ms/step - loss: 0.0031 - rmse: 0.0554 - val_loss: 0.0023 - val_rmse: 0.0477\n",
      "Epoch 93/100\n",
      "10/10 [==============================] - 5s 522ms/step - loss: 0.0028 - rmse: 0.0528 - val_loss: 0.0018 - val_rmse: 0.0429\n",
      "Epoch 94/100\n",
      "10/10 [==============================] - 5s 519ms/step - loss: 0.0030 - rmse: 0.0546 - val_loss: 0.0018 - val_rmse: 0.0424\n",
      "Epoch 95/100\n",
      "10/10 [==============================] - 5s 516ms/step - loss: 0.0033 - rmse: 0.0570 - val_loss: 0.0018 - val_rmse: 0.0420\n",
      "Epoch 96/100\n",
      "10/10 [==============================] - 5s 520ms/step - loss: 0.0029 - rmse: 0.0541 - val_loss: 0.0029 - val_rmse: 0.0541\n",
      "Epoch 97/100\n",
      "10/10 [==============================] - 5s 516ms/step - loss: 0.0032 - rmse: 0.0565 - val_loss: 0.0040 - val_rmse: 0.0634\n",
      "Epoch 98/100\n",
      "10/10 [==============================] - 5s 519ms/step - loss: 0.0034 - rmse: 0.0581 - val_loss: 0.0031 - val_rmse: 0.0558\n",
      "Epoch 99/100\n",
      "10/10 [==============================] - 5s 534ms/step - loss: 0.0036 - rmse: 0.0596 - val_loss: 0.0030 - val_rmse: 0.0550\n",
      "Epoch 100/100\n",
      "10/10 [==============================] - 5s 523ms/step - loss: 0.0030 - rmse: 0.0544 - val_loss: 0.0029 - val_rmse: 0.0537\n",
      "3/3 [==============================] - 1s 54ms/step\n",
      "Actual price: 149.71 - Predicted price: 144.22\n",
      "Actual price: 151.12 - Predicted price: 143.79\n",
      "Actual price: 146.55 - Predicted price: 140.70\n",
      "Actual price: 137.09 - Predicted price: 127.49\n",
      "Actual price: 106.84 - Predicted price: 113.97\n",
      "Actual price: 119.49 - Predicted price: 110.21\n",
      "Actual price: 151.28 - Predicted price: 147.30\n",
      "Actual price: 116.60 - Predicted price: 116.74\n",
      "Actual price: 175.74 - Predicted price: 160.47\n",
      "Actual price: 134.99 - Predicted price: 134.01\n",
      "Actual price: 164.77 - Predicted price: 153.89\n",
      "Actual price: 131.46 - Predicted price: 131.92\n",
      "Actual price: 115.56 - Predicted price: 111.83\n",
      "Actual price: 118.69 - Predicted price: 112.85\n",
      "Actual price: 174.33 - Predicted price: 161.08\n",
      "Actual price: 142.83 - Predicted price: 142.96\n",
      "Actual price: 148.85 - Predicted price: 143.43\n",
      "Actual price: 123.24 - Predicted price: 117.97\n",
      "Actual price: 143.29 - Predicted price: 142.35\n",
      "Actual price: 122.94 - Predicted price: 115.43\n",
      "Actual price: 92.85 - Predicted price: 100.71\n",
      "Actual price: 139.07 - Predicted price: 126.87\n",
      "Actual price: 119.98 - Predicted price: 123.77\n",
      "Actual price: 91.03 - Predicted price: 95.45\n",
      "Actual price: 115.98 - Predicted price: 113.50\n",
      "Actual price: 130.36 - Predicted price: 122.18\n",
      "Actual price: 113.90 - Predicted price: 99.14\n",
      "Actual price: 171.18 - Predicted price: 157.45\n",
      "Actual price: 97.72 - Predicted price: 97.59\n",
      "Actual price: 127.88 - Predicted price: 120.44\n",
      "Actual price: 133.41 - Predicted price: 130.66\n",
      "Actual price: 136.87 - Predicted price: 126.57\n",
      "Actual price: 142.81 - Predicted price: 141.44\n",
      "Actual price: 129.04 - Predicted price: 116.08\n",
      "Actual price: 116.87 - Predicted price: 116.29\n",
      "Actual price: 126.52 - Predicted price: 113.65\n",
      "Actual price: 120.96 - Predicted price: 120.59\n",
      "Actual price: 180.33 - Predicted price: 174.09\n",
      "Actual price: 116.59 - Predicted price: 117.46\n",
      "Actual price: 136.69 - Predicted price: 123.79\n",
      "Actual price: 125.89 - Predicted price: 127.51\n",
      "Actual price: 108.94 - Predicted price: 99.81\n",
      "Actual price: 114.91 - Predicted price: 110.01\n",
      "Actual price: 121.19 - Predicted price: 113.05\n",
      "Actual price: 126.11 - Predicted price: 126.69\n",
      "Actual price: 117.51 - Predicted price: 115.29\n",
      "Actual price: 115.01 - Predicted price: 107.99\n",
      "Actual price: 110.08 - Predicted price: 112.15\n",
      "Actual price: 161.84 - Predicted price: 157.15\n",
      "Actual price: 111.81 - Predicted price: 111.39\n",
      "Actual price: 128.70 - Predicted price: 121.15\n",
      "Actual price: 165.32 - Predicted price: 157.07\n",
      "Actual price: 146.92 - Predicted price: 146.36\n",
      "Actual price: 147.54 - Predicted price: 145.45\n",
      "Actual price: 95.75 - Predicted price: 96.47\n",
      "Actual price: 121.09 - Predicted price: 123.70\n",
      "Actual price: 120.88 - Predicted price: 120.03\n",
      "Actual price: 150.96 - Predicted price: 146.53\n",
      "Actual price: 121.10 - Predicted price: 113.32\n",
      "Actual price: 122.41 - Predicted price: 119.02\n",
      "Actual price: 148.99 - Predicted price: 146.11\n",
      "Actual price: 125.57 - Predicted price: 120.84\n",
      "Actual price: 153.49 - Predicted price: 147.75\n",
      "Actual price: 130.48 - Predicted price: 126.58\n",
      "Actual price: 127.90 - Predicted price: 122.41\n",
      "Actual price: 93.17 - Predicted price: 96.07\n",
      "Actual price: 133.11 - Predicted price: 131.43\n",
      "Actual price: 115.32 - Predicted price: 115.78\n",
      "Actual price: 126.85 - Predicted price: 125.86\n",
      "Actual price: 125.35 - Predicted price: 133.77\n",
      "Actual price: 115.81 - Predicted price: 108.38\n",
      "Actual price: 134.78 - Predicted price: 131.74\n",
      "Actual price: 122.54 - Predicted price: 121.90\n",
      "Actual price: 139.14 - Predicted price: 143.35\n",
      "Actual price: 123.75 - Predicted price: 114.98\n",
      "Actual price: 148.64 - Predicted price: 141.43\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "\n",
    "def load_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_data(data, lookback, days_ahead):\n",
    "    data = data[['Close']]\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(lookback, len(data) - days_ahead):\n",
    "        X.append(scaled_data[i - lookback:i, 0])\n",
    "        y.append(scaled_data[i + days_ahead, 0])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "    return X, y, scaler\n",
    "\n",
    "\n",
    "ticker = 'AAPL'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2021-12-31'\n",
    "lookback = 120\n",
    "days_ahead = 5\n",
    "\n",
    "# Load historical stock data\n",
    "data = load_data(ticker, start_date, end_date)\n",
    "\n",
    "# Preprocess the data\n",
    "X, y, scaler = preprocess_data(data, lookback, days_ahead)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(units=100, return_sequences=True, input_shape=(X_train.shape[1], 1))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(units=100)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Custom metric: Root Mean Squared Error\n",
    "rmse = RootMeanSquaredError(name='rmse')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[rmse])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "# Calculate the inverse transform of 'y_test'\n",
    "actual_prices = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Print the actual stock prices and their corresponding predictions\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"Actual price: {actual_prices[i][0]:.2f} - Predicted price: {predictions[i][0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5863746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
